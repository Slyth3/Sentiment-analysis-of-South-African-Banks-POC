{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Bank Sentiment Analysis \n",
    "\n",
    "## Summary \n",
    "Identify customer sentiment of the top 5 Banks in South Africa via twitter sentiment analysis scoring\n",
    "#### Operations:\n",
    "1. Twint to scrape tweets of the top 5 banks in South Africa \\ \n",
    "*  Standard Bank\n",
    "* Nedbank \n",
    "* Absa \n",
    "* FNB \n",
    "* Capitec\n",
    "\n",
    "2. Clean tweets with WordPunctTokenizer and Regex \n",
    "3. TextBlog to process sentiment of tweets \n",
    "5. Matplotlib / Seaborn to visualise and analyze\n",
    "\n",
    "#### Project 2:\n",
    "Create a custom model to identify Sentiment Analysis \n",
    "\n",
    "#### Project 3:\n",
    "Compare results to the Customer Satifaction Index (CSI) and determine if the CSI is a correct reflection of the consumer sentiment\n",
    "-> CSI is obtained by ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note \n",
    "This notebook is for analysis and confirmation of the process(the POC)\\\n",
    "The full customer built sentiment model is in a further notebook found on my github: https://github.com/Slyth3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twint  guide\n",
    "\n",
    "<b>My reference guide: </b>\n",
    "https://github.com/Slyth3/Twitter_NLP/blob/main/Quick%20Twint%20Code.txt\n",
    "\n",
    "<b> Official Github: </b>\n",
    "https://github.com/twintproject/twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint\n",
    "import pandas as pd\n",
    "import nest_asyncio             \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#optional: for reading and concatenation of previous files\n",
    "import glob                     \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "\n",
    "#cleaning\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords             \n",
    "\n",
    "# Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "#word cloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run Twint (twitter scrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for compatibility issues with twint\n",
    "nest_asyncio.apply()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_search = {\"FNB\":\"FNBSA\", \"StandardBank\":\"StandardBankZA OR \\\"Standard Bank\\\" OR \\\"standard bank\\\"\",\"Nedbank\":\"Nedbank OR nedbank\",\"ABSA\": \"Absa OR ABSA OR absa OR AbsaSouthAfrica\",\"Capitec\":\"CapitecBankSA OR Capitec or capitec\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twintConfig(date_from,date_to, search_string):\n",
    "    c = twint.Config()\n",
    "    c.Search = search_string[1]\n",
    "    c.Since = date_from\n",
    "    c.Until = date_to\n",
    "    c.Pandas = True\n",
    "    c. Pandas_au = True          \n",
    "    c.Pandas_clean=True\n",
    "    #c.Hide_output = True\n",
    "    #c.Resume = \"./ResumeID/resume_id_\"+search_string[0]+\".txt\"\n",
    "    twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = input(\"Input a start date eg 2021-09-17: \")\n",
    "until = input(\"Input an end date eg 2021-09-18: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Twint(search_vals):\n",
    "    \n",
    "    #set empty dataframe for join\n",
    "    out_df= pd.DataFrame()\n",
    "    \n",
    "    for bank in search_vals.items():\n",
    "        print (\"running for search item: \"+bank[0]+\"\\n\")\n",
    "        print (\"Search string: \"+bank[1]+\"\\n\")\n",
    "        \n",
    "        #run twint\n",
    "        twintConfig(since,until, bank)\n",
    "        \n",
    "        #get dataframe\n",
    "        tweets_df = twint.storage.panda.Tweets_df\n",
    "        \n",
    "        #join Dataframes and create 'Bank' column\n",
    "        tweets_df[\"Bank\"]= bank[0]\n",
    "        out_df = pd.concat([out_df,tweets_df])\n",
    "        \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df = Run_Twint(bank_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# precleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import/export file\n",
    "#base_tweets.to_csv(\"./Output/pre_cleaning____.csv\")\n",
    "tweets_df = pd.read_pickle(r\"C:\\Users\\andre\\OneDrive\\Desktop\\Personal\\Data Science\\Projects\\Bank_NLP_Twitter_POC - Copy\\Output\\precleaning_2021.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.drop([\"Unnamed: 0\", 'created_at', 'user_id_str', 'link', 'urls', 'photos', 'video',\n",
    "       'thumbnail', 'retweet','nreplies', 'nretweets', 'quote_url', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
    "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
    "       'trans_dest'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language analysis \n",
    "\n",
    "Although the language tag doesnt seem to get it right 100% of the time, we will drop these rows that arent english but keep undefined:\n",
    "* und = undefined --- this will also include tweets with only hashtags so we will keep this\n",
    "* en = english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_df[\"language\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows where language is not english or undefined\n",
    "tweets_df = tweets_df[tweets_df[\"language\"].isin([ 'und', 'en'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary rows \n",
    "* Remove tweets from Bank owned accounts i.e. FNBSA\n",
    "* Remove duplicates where tweet, bank and date are the same \n",
    "* Reindex dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where username is in bank_search\n",
    "tweets_df = tweets_df[ ~tweets_df[\"username\"].str.lower().str.contains('fnb|standardbank|nedbank|absa|capitec',regex = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicated tweets \n",
    "tweets_df = tweets_df.drop_duplicates(subset=['date',\"tweet\",\"Bank\"],keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index for visualisation later\n",
    "tweets_df.reset_index(inplace=True)\n",
    "tweets_df.drop(\"index\",axis =1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning tweet data \n",
    "* Remove punctuation, hashtags, symbols etc\n",
    "\n",
    "***Note:*** \n",
    "Cleaning will take a very long time depeding on your size of data and processing speeds \\\n",
    "In order to do parallel processing download the file (https://github.com/Slyth3/Sentiment-analysis-on-South-African-Banks/blob/main/multi_clean.py) \\\n",
    "Then import this file and run using the below:\n",
    "* import multi_clean\n",
    "* import multiprocessing as mp\n",
    "*from multiprocessing import  Pool\n",
    "* p = mp.Pool(mp.cpu_count())\n",
    "* cleaned_list = p.map(multi_clean.clean_text,base_tweets[\"tweet\"])\n",
    "* p.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):  \n",
    "    pat1 = r'@[^ ]+'                   #@signs\n",
    "    pat2 = r'https?://[A-Za-z0-9./]+'  #links\n",
    "    pat3 = r'\\'s'                      #floating s's\n",
    "    pat4 = r'\\#\\w+'                     # hashtags\n",
    "    pat5 = r'&amp '\n",
    "    pat6 = r'[^A-Za-z\\s]'         #remove non-alphabet\n",
    "    combined_pat = r'|'.join((pat1, pat2,pat3,pat4,pat5, pat6))\n",
    "    text = re.sub(combined_pat,\"\",text).lower()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tweets_df[\"cleaned_tweet\"] = tweets_df[\"tweet\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop empty rows\n",
    "tweets_df = tweets_df [ ~(tweets_df[\"tweet\"] ==\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"cleaned_tweet\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis (TextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Running sentiment process\")\n",
    "for row in tweets_df.itertuples():\n",
    "    tweet = tweets_df.at[row[0], 'cleaned_tweet']\n",
    "\n",
    "    #run sentiment using TextBlob\n",
    "    analysis = TextBlob(tweet)\n",
    "\n",
    "    #set value to dataframe\n",
    "    tweets_df.at[row[0], 'polarity'] = analysis.sentiment[0]\n",
    "    tweets_df.at[row[0], 'subjectivity'] = analysis.sentiment[1]\n",
    "\n",
    "    #Create Positive / negative column depending on polarity\n",
    "    if analysis.sentiment[0]>0:\n",
    "        tweets_df.at[row[0], 'Sentiment'] = \"Positive\"\n",
    "    elif analysis.sentiment[0]<0:\n",
    "        tweets_df.at[row[0], 'Sentiment'] = \"Negative\"\n",
    "    else:\n",
    "        tweets_df.at[row[0], 'Sentiment'] = \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[[\"cleaned_tweet\",\"polarity\",\"Sentiment\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import / Export\n",
    "#tweets_df = pd.read_pickle(r\"C:\\Users\\andre\\OneDrive\\Desktop\\Personal\\Data Science\\Projects\\Bank_NLP_Twitter_POC - Copy\\Output\\Final_2021.pickle\")\n",
    "#tweets_df.to_pickle(\"Final_2021.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create rolling Mean / Expanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"date\"] = pd.to_datetime(tweets_df[\"date\"])\n",
    "\n",
    "#set index = date so as to create rolling mean \n",
    "tweets_df = tweets_df.sort_values(\"date\").set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bank Dataframes \n",
    "Standard_df = tweets_df[(tweets_df.Bank==\"StandardBank\")]\n",
    "FNB_df = tweets_df[(tweets_df.Bank==\"FNB\")]\n",
    "Nedbank_df = tweets_df[(tweets_df.Bank==\"Nedbank\")]\n",
    "ABSA_df = tweets_df[(tweets_df.Bank==\"ABSA\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rolling/ expanding mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop this warning as the chaining is fine\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "Nedbank_df['mean'] = Nedbank_df['polarity'].expanding().mean()\n",
    "Nedbank_df['rolling'] = Nedbank_df['polarity'].rolling(\"7d\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Std Bank\n",
    "Standard_df['mean'] = Standard_df['polarity'].expanding().mean()\n",
    "Standard_df['rolling'] = Standard_df['polarity'].rolling(\"7d\").mean()\n",
    "\n",
    "#FNB\n",
    "FNB_df['mean'] = FNB_df['polarity'].expanding().mean()\n",
    "FNB_df['rolling'] = FNB_df['polarity'].rolling(\"7d\").mean()\n",
    "\n",
    "#Nebank\n",
    "Nedbank_df['mean'] = Nedbank_df['polarity'].expanding().mean()\n",
    "Nedbank_df['rolling'] = Nedbank_df['polarity'].rolling(\"7d\").mean()\n",
    "\n",
    "#ABSA\n",
    "ABSA_df['mean'] = ABSA_df['polarity'].expanding().mean()\n",
    "ABSA_df['rolling'] = ABSA_df['polarity'].rolling(\"7d\").mean()\n",
    "\n",
    "#Capitec\n",
    "Cap_df['mean'] = Cap_df['polarity'].expanding().mean()\n",
    "Cap_df['rolling'] = Cap_df['polarity'].rolling(\"7d\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install additional libraries for visualisation \n",
    "import ast #optional\n",
    "from collections import Counter\n",
    "\n",
    "import cufflinks as cf\n",
    "from plotly.offline import init_notebook_mode #, plot, iplot, download_plotlyjs\n",
    "init_notebook_mode(connected = True)\n",
    "cf.go_offline()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pallette \n",
    "sns.set_theme()\n",
    "pal = {\"FNB\":'c', \"StandardBank\":\"b\",\"ABSA\":\"r\",\"Nedbank\":\"g\", \"Capitec\": \"grey\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize= (15,5))\n",
    "\n",
    "sns.countplot(ax = ax[0], x= tweets_df[\"Bank\"], palette= pal)\n",
    "ax[0].set_title(\"Count of tweets\")\n",
    "\n",
    "sns.barplot(data =tweets_df, x = \"Bank\" ,y = \"nlikes\",estimator=np.sum,ci=None, palette=pal)\n",
    "ax[1].set_title(\"Count of likes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tweets\n",
    "tweets_df[[\"cleaned_tweet\",\"Bank\"]].groupby([\"Bank\"]).count().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "sns.histplot(tweets_df, x=\"Sentiment\", hue=\"Bank\", palette= pal,multiple=\"stack\", alpha = 1)\n",
    "plt.title(\"Count of tweets by sentiment\",fontsize =15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = sns.displot(tweets_df, x=\"Sentiment\", col=\"Bank\",col_wrap= 2, hue=\"Bank\", legend=False, palette= pal)\n",
    "fig1.fig.suptitle(\"Count of tweets by Sentiment\",fontsize =15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = sns.displot(data = tweets_df[~(tweets_df['polarity']==0)], x=\"polarity\",\n",
    "                   col=\"Bank\",\n",
    "                   col_wrap= 2, \n",
    "                   hue=\"Bank\", \n",
    "                   legend=False, \n",
    "                   palette= pal,\n",
    "                   kde = True,\n",
    "                   bins =30)\n",
    "fig1.fig.suptitle(\"Distribution of Sentiment scores(polarity)\",fontsize =15 )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtag analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all hashtags as list\n",
    "def hashlist(df):\n",
    "    hashlist = []\n",
    "    for i in df['hashtags']:\n",
    "        #use ast.literal if you are importing CSV files otherwise just use 'i'\n",
    "        hashlist.extend(ast.literal_eval(i))\n",
    "    return hashlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create hashtag dataframes\n",
    "hash_Absa= pd.DataFrame(Counter(hashlist(ABSA_df)).items()).sort_values(1,ascending=False)\n",
    "hash_NedBank= pd.DataFrame(Counter(hashlist(Nedbank_df)).items()).sort_values(1,ascending=False)\n",
    "hash_StdBank= pd.DataFrame(Counter(hashlist(Standard_df)).items()).sort_values(1,ascending=False)\n",
    "hash_FNB= pd.DataFrame(Counter(hashlist(FNB_df)).items()).sort_values(1,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2,figsize=(15, 10))\n",
    "\n",
    "plt.suptitle(\"Top 5 hashtags per bank\")\n",
    "\n",
    "# ABSA\n",
    "ax[0,0].bar(hash_Absa[0].head(), hash_Absa[1].head(), color = \"r\")\n",
    "ax[0,0].set_title(\"ABSA\")\n",
    "ax[0,0].xaxis.set_tick_params(rotation=45, size = 15)\n",
    "\n",
    "ax[0,1].bar(hash_NedBank[0].head(), hash_NedBank[1].head(), color = \"g\")\n",
    "ax[0,1].set_title(\"Nedbank\")\n",
    "ax[0,1].xaxis.set_tick_params(rotation=45, size = 15)\n",
    "\n",
    "ax[1,0].bar(hash_StdBank[0].head(), hash_StdBank[1].head(), color = \"b\")\n",
    "ax[1,0].set_title(\"Standard Bank\")\n",
    "ax[1,0].xaxis.set_tick_params(rotation=45, size = 15)\n",
    "\n",
    "ax[1,1].bar(hash_FNB[0].head(), hash_FNB[1].head(), color = \"c\")\n",
    "ax[1,1].set_title(\"FNB\")\n",
    "ax[1,1].xaxis.set_tick_params(rotation=45, size = 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetString_a = \" \".join(list(ABSA_df[\"cleaned_tweet\"])).lower()\n",
    "tweetString_n = \" \".join(list(Nedbank_df[\"cleaned_tweet\"])).lower()\n",
    "tweetString_s = \" \".join(list(Standard_df[\"cleaned_tweet\"])).lower()\n",
    "tweetString_f = \" \".join(list(FNB_df[\"cleaned_tweet\"])).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Bank name and set wordcloud\n",
    "\n",
    "tweetString_a = re.sub(r\"absa|bank|amp\",\"\",tweetString_a)\n",
    "wordcloud_a = WordCloud(\n",
    "                background_color ='white', \n",
    "                min_font_size = 5).generate(tweetString_a)\n",
    "\n",
    "tweetString_n = re.sub(r\"NedBankSA|Nedbank|nedbank|bank|amp\",\"\",tweetString_n)   \n",
    "wordcloud_n = WordCloud( \n",
    "                background_color ='white', \n",
    "                min_font_size = 5).generate(tweetString_n)\n",
    "\n",
    "tweetString_s = re.sub(r\"standardbankza|standard bank|bank|amp\",\"\",tweetString_s)     \n",
    "wordcloud_s = WordCloud( \n",
    "                background_color ='white', \n",
    "                min_font_size = 5).generate(tweetString_s)\n",
    "\n",
    "tweetString_f = re.sub(r\"FNB|fnb|bank|amp\",\"\",tweetString_f)\n",
    "wordcloud_f = WordCloud( \n",
    "                background_color ='white', \n",
    "                min_font_size = 5).generate(tweetString_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(14, 8),sharey=True)\n",
    "\n",
    "ax[0,0].imshow(wordcloud_s)\n",
    "ax[0,1].imshow(wordcloud_f)\n",
    "ax[1,0].imshow(wordcloud_n)\n",
    "ax[1,1].imshow(wordcloud_a)\n",
    "\n",
    "ax[0,0].axis(\"off\")\n",
    "ax[0,1].axis(\"off\")\n",
    "ax[1,0].axis(\"off\")\n",
    "ax[1,1].axis(\"off\")\n",
    "\n",
    "ax[0,0].set_title(\"StandardBank\")\n",
    "ax[0,1].set_title(\"FNB\")\n",
    "ax[1,0].set_title(\"Nedbank\")\n",
    "ax[1,1].set_title(\"ABSA\")\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall mean sentiment by bank\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Overall mean Sentiment by Bank\")\n",
    "sns.barplot(data = tweets_df, x= \"Bank\", y = \"polarity\", palette=pal, ci=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to create our graphs\n",
    "def trace_rolling_creation(df,gname, glinecolor):\n",
    "    return fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x= df.index, \n",
    "            y=df[\"rolling\"], \n",
    "            name=gname,  \n",
    "            mode='lines',\n",
    "            line_color=glinecolor),\n",
    "        secondary_y=False\n",
    ")\n",
    "\n",
    "def trace_count_creation(df,gname, glinecolor):\n",
    "    return fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x= df.index, \n",
    "            y=df[\"polarity\"].rolling('7d').count(), \n",
    "            name=gname,  \n",
    "            fill='tozeroy',line_color=glinecolor), \n",
    "        secondary_y=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_scatter(x=FNB_df.index, y=FNB_df[\"rolling\"], name=\"FNB\", mode='lines',line_color=\"#19D3F3\")\n",
    "fig.add_scatter(x=Standard_df.index, y=Standard_df[\"rolling\"], name=\"Standard Bank\", mode='lines',line_color=\"blue\")\n",
    "fig.add_scatter(x=ABSA_df.index, y=ABSA_df[\"rolling\"], name=\"ABSA\", mode='lines',line_color=\"red\")\n",
    "fig.add_scatter(x=Nedbank_df.index, y=Nedbank_df[\"rolling\"], name=\"Nedbank\", mode='lines',line_color=\"green\")\n",
    "fig.update_layout(\n",
    "    template = \"seaborn\",\n",
    "    title=\"Rolling 7 day Sentiment (polarity)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"7 day rolling polarity\",\n",
    "    yaxis_range = [-0.1,0.4],\n",
    "    legend_title=\"Banks\",\n",
    "    font=dict(size=12),\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    margin=dict(l=10,r=10, b=50,t=50, pad=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "trace_rolling_creation(ABSA_df, \"ABSA\", '#DC0E1A')\n",
    "trace_rolling_creation(Nedbank_df, \"Nedbank\", '#078a4d')\n",
    "trace_rolling_creation(Standard_df, \"StdBank\", '#054db3')\n",
    "trace_rolling_creation(FNB_df, \"FNB\", '#19D3F3')\n",
    "\n",
    "trace_count_creation(ABSA_df, \"ABSA\", 'rgb(220, 14, 26)')\n",
    "trace_count_creation(Nedbank_df, \"NedBank\", 'rgb(7, 138, 77)')\n",
    "trace_count_creation(Standard_df, \"Std Bank\", 'rgb(5, 77, 179)')\n",
    "trace_count_creation(FNB_df, \"FNB\", 'rgb(25, 211, 243)')\n",
    "# set figure layout\n",
    "fig.update_layout(\n",
    "    template = \"seaborn\",\n",
    "    title_text=\"Rolling 7d Sentiment vs Count of tweets\",\n",
    "    legend_title=\"Banks\",\n",
    "    font=dict(size=12),\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    margin=dict (l=10,r=10,b=50,t=50, pad=2)\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Date\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Rolling\",range = [-0.1,0.4], secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Count\",range = [0,40000], secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day / Month sentiment comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create day and month\n",
    "ABSA_df[\"Day\"]= ABSA_df.index.day_name()\n",
    "ABSA_df[\"Month\"] = ABSA_df.index.month_name()\n",
    "ABSA_df[\"Hour\"] = ABSA_df.index.hour\n",
    "Nedbank_df[\"Day\"]= Nedbank_df.index.day_name()\n",
    "Nedbank_df[\"Month\"] = Nedbank_df.index.month_name()\n",
    "Nedbank_df[\"Hour\"] = Nedbank_df.index.hour\n",
    "Standard_df[\"Day\"]= Standard_df.index.day_name()\n",
    "Standard_df[\"Month\"] = Standard_df.index.month_name()\n",
    "Standard_df[\"Hour\"] = Standard_df.index.hour\n",
    "FNB_df[\"Day\"]= FNB_df.index.day_name()\n",
    "FNB_df[\"Month\"] = FNB_df.index.month_name()\n",
    "FNB_df[\"Hour\"] = FNB_df.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sort Day and month columns\n",
    "from pandas.api.types import CategoricalDtype\n",
    "days = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_type = CategoricalDtype(categories=days, ordered=True)\n",
    "\n",
    "ABSA_df['Day'] = ABSA_df['Day'].astype(day_type)\n",
    "Nedbank_df['Day'] = Nedbank_df['Day'].astype(day_type)\n",
    "Standard_df['Day'] = Standard_df['Day'].astype(day_type)\n",
    "FNB_df['Day'] = FNB_df['Day'].astype(day_type)\n",
    "\n",
    "months = [ 'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "month_type = CategoricalDtype(categories=months, ordered=True)\n",
    "ABSA_df['Month'] = ABSA_df['Month'].astype(month_type)\n",
    "Nedbank_df['Month'] = Nedbank_df['Month'].astype(month_type)\n",
    "Standard_df['Month'] = Standard_df['Month'].astype(month_type)\n",
    "FNB_df['Month'] = FNB_df['Month'].astype(month_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "sns.lineplot(data = FNB_df.groupby(\"Month\")[\"polarity\"].mean(), color = \"c\", label = \"FNB\")\n",
    "sns.lineplot(data = Nedbank_df.groupby(\"Month\")[\"polarity\"].mean(), color = \"g\", label = \"Nedbank\")\n",
    "sns.lineplot(data = ABSA_df.groupby(\"Month\")[\"polarity\"].mean(), color = \"r\", label = \"ABSA\")\n",
    "sns.lineplot(data = Standard_df.groupby(\"Month\")[\"polarity\"].mean(), color = \"b\", label = \"StdBank\")\n",
    "plt.title(\"Sentiment by month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Sentiment by day\")\n",
    "sns.lineplot(data = FNB_df.groupby(\"Day\")[\"polarity\"].mean(), color = \"c\", label = \"FNB\", sort=False)\n",
    "sns.lineplot(data = Nedbank_df.groupby(\"Day\")[\"polarity\"].mean(), color = \"g\", label = \"Nedbank\", sort=False)\n",
    "sns.lineplot(data = ABSA_df.groupby(\"Day\")[\"polarity\"].mean(), color = \"r\", label = \"ABSA\", sort=False)\n",
    "sns.lineplot(data = Standard_df.groupby(\"Day\")[\"polarity\"].mean(), color = \"b\", label = \"StdBank\", sort=False)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Sentiment by hour\")\n",
    "sns.lineplot(data = FNB_df.groupby(\"Hour\")[\"polarity\"].mean(), color = \"c\", label = \"FNB\")\n",
    "sns.lineplot(data = Nedbank_df.groupby(\"Hour\")[\"polarity\"].mean(), color = \"g\", label = \"Nedbank\")\n",
    "sns.lineplot(data = ABSA_df.groupby(\"Hour\")[\"polarity\"].mean(), color = \"r\", label = \"ABSA\")\n",
    "sns.lineplot(data = Standard_df.groupby(\"Hour\")[\"polarity\"].mean(), color = \"b\", label = \"StdBank\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
