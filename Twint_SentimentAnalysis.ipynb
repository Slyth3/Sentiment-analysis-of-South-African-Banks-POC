{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Bank Analysis - Tweets \n",
    "Using Twint to scrape tweets \n",
    "\n",
    "Use beautiful soup to clean tweets with WordPunctTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Twint code\n",
    "Github - https://github.com/twintproject/twint\n",
    "\n",
    "All functions - https://github.com/twintproject/twint/wiki/Configuration\n",
    "### setup \n",
    "c.twint.Config() \n",
    "\n",
    "#### set username\n",
    "c.Username = \"realDonaldTrump\"\n",
    "\n",
    "#### set phrase search\n",
    "c.Search = \"great\"\n",
    "\n",
    "#### customise output\n",
    "c.Custom[\"tweet\"] = [\"id\"]           --- assign column names\n",
    "\n",
    "c.Custom[\"user\"] = [\"bio\"]\n",
    "\n",
    "c.Limit = 1                          ---- limit to batches (unknown size)\n",
    "\n",
    "c.Since = \"2019–04–29\"\n",
    "\n",
    "c.Until = \"2020–04–29\"\n",
    "\n",
    "#### Pandas \n",
    "c.Pandas = True\n",
    "\n",
    "    --------once run save to dataframe\n",
    "    \n",
    "    ---- Tweets \n",
    "    df = twint.storage.panda.Tweets_df\n",
    "    \n",
    "    ---- followers \n",
    "    df = twint.storage.panda.Follow_df\n",
    "    \n",
    "    df = Followers_df['followers'][username]\n",
    "\n",
    "#### Write output \n",
    "c.Store_csv = True\n",
    "\n",
    "c.Output = \"test.csv\"\n",
    "\n",
    "### Execute determines how its run- using setup above\n",
    "twint.run.Search(c)    --- will run search for all tweets with the above\n",
    "\n",
    "twint.run.Profile(c)   ---- will run against profile - return only this profiles tweets\n",
    "\n",
    "twint.run.Followers(c) ---- get follower info \n",
    "\n",
    "\n",
    "config.Since = \"2019–04–29\"\\\n",
    "config.Until = \"2020–04–29\"\\\n",
    "config.Store_json = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import twint\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()            #for compatibility issues \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "\n",
    "#cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# NLP\n",
    "from textblob import TextBlob\n",
    "#from IPython.display import Markdown, display\n",
    "\n",
    "#word cloud and stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and run Twint (twitter scrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankString = \"StandardBankZA OR \\\"Standard Bank\\\"\"\n",
    "since = \"2021-05-27\"\n",
    "\n",
    "def twintRun(date_from,search_string):    \n",
    "    c = twint.Config()\n",
    "    c.Search = search_string\n",
    "    c.Pandas = True\n",
    "    c.Since = date_from\n",
    "    #c.Until = \"2021-07-30\"\n",
    "    print(\"running \\n\")\n",
    "    twint.run.Search(c)\n",
    "    print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "twintRun(since,bankString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas - analyse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = twint.storage.panda.Tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to csv\n",
    "tweets_df.to_csv(\"pre_cleaning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tweets = tweets_df[[\"date\", \"username\", \"tweet\", \"hashtags\", \"nlikes\",\"search\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning tweet data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_patterns(text):  \n",
    "    pat1 = r'@[A-Za-z0-9]+'\n",
    "    pat2 = r'https?://[A-Za-z0-9./]+'  \n",
    "    pat3 = r'\\#\\w+'                    #hashtags - already captured by Twint\n",
    "    pat4 = r\"\\'s\"                      #floating s's\n",
    "    combined_pat = r'|'.join((pat1, pat2,pat3,pat4))\n",
    "    text = re.sub(combined_pat,\"\",text)\n",
    "    return text\n",
    "\n",
    "def clean_text(text): \n",
    "    text=remove_content(text)\n",
    "    \n",
    "    text = re.sub('[^A-Za-z\\s]', '', text.lower())        #remove non-alphabets, but ignire\n",
    "    tokenized_text = WordPunctTokenizer().tokenize(text) #tokenize\n",
    "    clean_text = [\n",
    "         word for word in tokenized_text\n",
    "         if word not in STOPWORDS\n",
    "    ]\n",
    "    return ' '.join(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run tween cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_to_clean = sub_tweets[\"tweet\"]\n",
    "cleaned_list = []\n",
    "\n",
    "for t in Data_to_clean:\n",
    "    cleaned_list.append(clean_text(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##make a dataframe for sentiment analysis\n",
    "clean_df = pd.DataFrame(cleaned_list,columns=['tweet'])\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in clean_df.iterrows():\n",
    "    tweet = clean_df.at[index, 'tweet']\n",
    "\n",
    "    #run sentiment using TextBlob\n",
    "    analysis = TextBlob(tweet)\n",
    "\n",
    "    #set value to dataframe\n",
    "    clean_df.at[index, 'polarity'] = analysis.sentiment[0]\n",
    "    clean_df.at[index, 'subjectivity'] = analysis.sentiment[1]\n",
    "\n",
    "\n",
    "    #Create Positive / negative column depending on polariity\n",
    "    if analysis.sentiment[0]>0:\n",
    "\n",
    "        clean_df.at[index, 'Sentiment'] = \"Positive\"\n",
    "        #printmd('Positive', color=\"green\")\n",
    "\n",
    "    elif analysis.sentiment[0]<0:\n",
    "\n",
    "        clean_df.at[index, 'Sentiment'] = \"Negative\"\n",
    "    else:\n",
    "\n",
    "        clean_df.at[index, 'Sentiment'] = \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"cleaned.csv\")\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge the dataframes - to get date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df = clean_df.merge(sub_tweets, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df = Final_df.rename(columns={\"tweet_x\": \"cleaned_tweet\", \"tweet_y\": \"Base_tweet\"})\n",
    "Final_df.to_csv(\"check.csv\")\n",
    "Final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure date is seen as datetime\n",
    "Final_df[\"date\"] = pd.to_datetime(Final_df[\"date\"])\n",
    "\n",
    "#set index = date so as to create rolling and expanding mean \n",
    "Final_df.index = pd.to_datetime(Final_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df['mean'] = Final_df['polarity'].expanding().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df['rolling'] = Final_df['polarity'].rolling(\"1d\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df[[\"polarity\",\"mean\",\"rolling\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "sns.histplot(Final_df['polarity'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### remove neutral values \n",
    "#Final_df = Final_df[Final_df.polarity != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetString = \" \".join(list(Final_df[\"cleaned_tweet\"])).lower()\n",
    "tweetString1 = re.sub(r\"standardbankza|standard bank|bank\",\"\",tweetString)     #remove bank name\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 500, \n",
    "                background_color ='white', \n",
    "                min_font_size = 5).generate(tweetString1)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative_df = Final_df[(Final_df.Sentiment==\"Negative\")]\n",
    "NegativeString = \" \".join(list(Negative_df[\"cleaned_tweet\"])).lower()\n",
    "tweetString1 = re.sub(r\"standardbankza|standard bank|bank\",\"\",tweetString)     #remove bank name\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 500, \n",
    "                background_color ='white', \n",
    "                min_font_size = 5).generate(tweetString1)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NegativeString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(Final_df['date'],Final_df['rolling'], color ='r', label='Rolling Mean')\n",
    "ax.plot(Final_df['date'],Final_df['mean'], color='y', label='Expanding Mean')\n",
    "#z= plt.plot(Final_df['date'],Final_df[\"polarity\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "#plot data \n",
    "ax.scatter(Final_df['date'],Final_df['polarity'], label='Sentiment')\n",
    "ax.plot(Final_df['date'],Final_df['rolling'], color ='r', label='Rolling Mean')\n",
    "ax.plot(Final_df['date'],Final_df['mean'], color='y', label='Expanding Mean')\n",
    "ax.set(title='STD Bank Tweet Sentiment', xlabel='Date', ylabel='Sentiment')\n",
    "\n",
    "# set font and rotation for date tick labels\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "ax.legend(loc='best')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df.to_csv(\"NLP_Standardbank_July.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative_df = Final_df[(Final_df.Sentiment==\"Negative\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
